{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st DNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0gOpQYQviLF",
        "colab_type": "text"
      },
      "source": [
        "# Version-1\n",
        "- Simple vanilla network with no Dropout, no BatchNormalization, no variable Learning Rate, no larger batch size, and only Adam optimizer\n",
        "- Summary:\n",
        "  - Total Parameters: 500,554\n",
        "  - First epoch validation accuracy: 98.27%\n",
        "  - Max validation accuracy in 25 epochs: 99.27% (in 4th epoch)\n",
        "  - Batch Size: 32\n",
        "- The purpose of this version is to decide the model architecture and number of layers.\n",
        "- Number of layers are decided based on the **Receptive Field**\n",
        "- The input image size is 28x28. Input images contains hand written digits which are around 26x26 so receptive field of 26x26 is needed to have better classification results\n",
        "- Before 2014, it was a common practise to follow the below mentioned model architecture\n",
        "![](https://github.com/Shilpaj1994/DeepVision/blob/master/Assignment-4/data/2014.png?raw=true)\n",
        "- After 2014, the fully connected layers in the above architecture were replaced by 1x1 convolutions\n",
        "- After 2016, all the modern architectures started using blocks in the network like convolution block, transition block, output block, etc.\n",
        "- For this version, the architecture is designed in such a way that it contains 2-convolution blocks, 2-transition blocks and 1-output block  \n",
        "![](https://github.com/Shilpaj1994/DeepVision/blob/master/Assignment-4/data/model.png?raw=true)  *Diagram only for reference*\n",
        "\n",
        "- **Convolution Block:**\n",
        "  - It contains number of convolution layer with increasing number of kernels\n",
        "  - The idea behind this is, as we decrease dimension of the image, we increase number of kernels to extract more complex information to compensate for the loss of information\n",
        "  \n",
        "- **Transition Block:**\n",
        "  - It contains 1x1 convolution layer with/without MaxPooling layer\n",
        "  - The 1x1 convolution is used to combine the features in the channels and carry forward only important features\n",
        "  - This helps in reducing the number of channel\n",
        "  \n",
        "- **Output Block:**\n",
        "  - It contains convolution layer without activation. The activation is not provided since softmax activation is present for classification\n",
        "  - It reduces the output number of channels equal to number of classes in the dataset\n",
        "  - Flatten layer convert the multi-dimensional data into a vector\n",
        "  - Softmax activation gives classification of the input image.\n",
        "- 98.27% validation accuracy in first epoch shows that it is a potentially good model to work on.\n",
        "- Thus, the model architecture is decided in this version.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "### Install Dependencies \n",
        "\n",
        "- To train the following network, we use a framework named Keras.\n",
        "- Keras provides functions for Convolution layers, Activation layers, MaxPooling layer, etc. so we don't need write code for designing such layers. Instead we can focus on creating better network architecture\n",
        "- Following lines of code installs Keras on the system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_6H50FBVsld",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries and Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Numpy for matrices and multi-dimensional array's processing\n",
        "import numpy as np\n",
        "\n",
        "# Import Sequential to write model layer-by-layer in sequence \n",
        "from keras.models import Sequential\n",
        "\n",
        "# Import Flatten layer to flatten feature-map, Dropout to avoid overfitting\n",
        "from keras.layers import Flatten, Dropout, Activation\n",
        "\n",
        "# Import Convolution layer to perform convolution on the channels, MaxPooling to reduce dimensions of channel\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "\n",
        "# Import np_utils for one-hot-encoding\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Import hand written dataset of numbers from 0-9\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data\n",
        "The data is loaded in following variables:\n",
        "\n",
        "- X_train: Samples used during training the network\n",
        "- y_train: Corresponding labels for training data\n",
        "- X_test: Samples used for validation after training the network\n",
        "- y_test: Corresponding labels for the validation of network's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxIXgp8xYNXF",
        "colab_type": "text"
      },
      "source": [
        "### Dataset Information and Display Data Sample\n",
        "- We have 60000 images in training dataset\n",
        "- We have 10000 images in the testing dataset\n",
        "- Each image's dimension are 28x28x1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "642aa026-cac0-4131-f644-6e9f1eeb9392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# Dimensions of the training dataset images\n",
        "print (X_train.shape)\n",
        "\n",
        "# Dimensions of the testing dataset images\n",
        "print (X_test.shape)\n",
        "\n",
        "# Import python module for plotting the image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Below line is written to display an image in this notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Plotting first image in the training dataset\n",
        "# cmap='gray' displays the data sample in appropriate color space\n",
        "plt.imshow(X_train[0], cmap='gray')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f524ab1e0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKorE7Q0ZOZY",
        "colab_type": "text"
      },
      "source": [
        "### Reshape the dataset\n",
        "- Keras requires the input data in a form of 4D tensor\n",
        "- The first value represents the total number of images in a training/testing dataset\n",
        "- Second and third values are dimensions of an image\n",
        "- Fourth value is the number of channels (1 for grayscale and 3 for RGB)  \n",
        "\n",
        "Thus, the X_train will have a shape of (60000, 28, 28, 1)  \n",
        "and the X_test will have a shape of (1000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yaQcS6aZnte",
        "colab_type": "text"
      },
      "source": [
        "### Normalizing the Data\n",
        "- When a Kernel is convolved over an input image, the maximum pixel value in the feature-map depends upon the maximum pixel value in the kernel\n",
        "- Different kernels will have different maximum values and so their corresponding feature-maps will have different maximum values\n",
        "- The feature-map with greater maximum pixel value will be louder while training the network\n",
        "- To avoid biased activations of such kernels, we perform Normalization\n",
        "- For normalization, we first convert the data into float so that we can get all the decimal values\n",
        "- By dividing all the pixels by 255, all the pixel values will be restricted between 0.0 to 1.0. This is how we normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzfX0QqZ9HS",
        "colab_type": "text"
      },
      "source": [
        "### Print Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "64cb71bc-4de7-4ccb-ab0b-794606df9edf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhFS2uM3aDEh",
        "colab_type": "text"
      },
      "source": [
        "### One-Hot Encoding\n",
        "- It is way of representing labels\n",
        "- Instead of using one-single scalar for labels, we use a vector to represent the labels.\n",
        "- The position of the ground-truth is marked as 1 while other positions are marked as 0\n",
        "- The network cannot print out the prediction as 0,1,2,....9\n",
        "- Instead it can activate the neuron associated with these numbers. So, the last layer before activation layer has number of neurons equal to number of classes (in this case 10)\n",
        "- The neuron associated with the number is set as 1 while other neurons are set as 0. Following is the pattern in which encoding is done:  \n",
        "  - Number 0 is encoded as 1000000000  \n",
        "  - Number 1 is encoded as 0100000000  \n",
        "  - Number 2 is encoded as 0010000000  \n",
        "   .  \n",
        "   .  \n",
        "   .  \n",
        "   .  \n",
        "  - Number 9 is encoded as 0000000001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yShor8IUaiT9",
        "colab_type": "text"
      },
      "source": [
        "### Print Labels after one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "832f2c16-f1f9-409f-edb4-a0a812483075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spiB8iJDao-w",
        "colab_type": "text"
      },
      "source": [
        "### Model Architecture\n",
        "\n",
        "- Model is defined sequential\n",
        "- The model has convolution, maxpooling, flatten and softmax layers\n",
        "\n",
        "- **Convolution Layer:**\n",
        "  - It is a process of extracting features from a channel using a kernel (feature extractor)\n",
        "![](https://github.com/Shilpaj1994/Phase1_assignments/blob/master/Assignment%201/5-3ConvolutionSmall.gif?raw=true)\n",
        "\n",
        "- **MaxPooling Layer:**\n",
        "  - It reduces the dimension of an channel. If we use MaxPooling of 2x2, dimension of an channel will become half of input channel\n",
        "  - It only passes the louder pixel value in the next layer\n",
        "![](https://github.com/Shilpaj1994/Phase1_assignments/blob/master/Assignment%203/Files/maxpool.gif?raw=true)\n",
        "\n",
        "- **Softmax Layer:**\n",
        "  - It is like probability\n",
        "  - It gives score of a class between 0 and 1\n",
        "![](https://github.com/Shilpaj1994/Phase1_assignments/blob/master/Assignment%201/softmax.png?raw=true)\n",
        "\n",
        "- **Flatten Layer:**\n",
        "  - It flattens the input dimension\n",
        "  - Multiple 2D channels are converted into a vector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "634ea643-1d4f-4363-da68-5b74d9376b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        }
      },
      "source": [
        "# For reproducable results\n",
        "np.random.seed(7)\n",
        "\n",
        "# Define Sequential Model Type\n",
        "model = Sequential()\n",
        "\n",
        "# Convolution Block\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu', input_shape=(28,28,1), name='conv_1'))   # Layer 1: Input:28x28x01  |  Kernels:(3x3x01)x32  |  Output:26x26x32  |  Receptive Field:3x3 \n",
        "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv_2'))                          # Layer 2: Input:26x26x32  |  Kernels:(3x3x32)x64  |  Output:24x24x64  |  Receptive Field:5x5  \n",
        "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv_3'))                         # Layer 3: Input:24x24x64  |  Kernels:(3x3x64)x128 |  Output:22x22x128 |  Receptive Field:7x7\n",
        "\n",
        "# Transition Block\n",
        "model.add(Convolution2D(32, 1, 1, activation='relu', name='conv_4_1x1'))                      # Layer 4: Input:22x22x128 |  Kernels:(1x1x16)x32  |  Output:22x22x32  |  Receptive Field:7x7 \n",
        "model.add(MaxPooling2D(2, name='MP'))                                                         # Layer 5: Input:22x22x32  |    MaxPooling:(2x2)   |  Output:11x11x32  |  Receptive Field:14x14 \n",
        "\n",
        "# Convolution Block\n",
        "model.add(Convolution2D(64, 3, 3, activation='relu', name='conv_5'))                          # Layer 6: Input:11x11x32  |  Kernels:(3x3x32)x64  |  Output:9x9x64  |  Receptive Field:16x16 \n",
        "model.add(Convolution2D(128, 3, 3, activation='relu', name='conv_6'))                         # Layer 7: Input:09x09x64  |  Kernels:(3x3x64)x128 |  Output:7x7x128 |  Receptive Field:18x18 \n",
        "model.add(Convolution2D(256, 3, 3, activation='relu', name='conv_7'))                         # Layer 8: Input:07x07x128 | Kernels:(3x3x128)x256 |  Output:5x5x256 |  Receptive Field:20x20 \n",
        "\n",
        "# Transition-ish Block\n",
        "model.add(Convolution2D(32, 1, 1, activation='relu', name='conv_8_1x1'))                      # Layer 9: Input:5x5x256   |  Kernels:(1x1x256)x32 |  Output:5x5x32  |  Receptive Field:22x22\n",
        "\n",
        "# Output Block\n",
        "model.add(Convolution2D(10, 5, name='conv_9'))                                                # Layer 10: Input:5x5x32   |  Kernels:(5x5x32)x10  |  Output:1x1x10  |  Receptive Field:27x27 \n",
        "model.add(Flatten())                                                                          # Layer 11: Input:1x1x10   |  Output:10\n",
        "model.add(Activation('softmax'))                                                              # Layer 12: Activation Layer\n",
        "\n",
        "# Summaries above architecture\n",
        "model.summary()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_1 (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv_3 (Conv2D)              (None, 22, 22, 128)       73856     \n",
            "_________________________________________________________________\n",
            "conv_4_1x1 (Conv2D)          (None, 22, 22, 32)        4128      \n",
            "_________________________________________________________________\n",
            "MP (MaxPooling2D)            (None, 11, 11, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv_5 (Conv2D)              (None, 9, 9, 64)          18496     \n",
            "_________________________________________________________________\n",
            "conv_6 (Conv2D)              (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv_7 (Conv2D)              (None, 5, 5, 256)         295168    \n",
            "_________________________________________________________________\n",
            "conv_8_1x1 (Conv2D)          (None, 5, 5, 32)          8224      \n",
            "_________________________________________________________________\n",
            "conv_9 (Conv2D)              (None, 1, 1, 10)          8010      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 500,554\n",
            "Trainable params: 500,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1..., name=\"conv_1\")`\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv_2\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv_3\")`\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"conv_4_1x1\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", name=\"conv_5\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"conv_6\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", name=\"conv_7\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"conv_8_1x1\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PEwIVMbZFI",
        "colab_type": "text"
      },
      "source": [
        "### Compile Model\n",
        "- **Loss Function:**\n",
        "  - Loss function is used to calculate the error  between the prediction and actual label\n",
        "  - Using optimizer, we reduce the value of loss function as much as possible\n",
        "  - [More about cross-entropy loss](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8)\n",
        "\n",
        "- **Optimizer:**\n",
        "  - Optimizers decide by how much value the weights should be changed while training the network \n",
        "  - Adam (Adaptive Moment Estimation) uses a complicated exponential decay that consists of the average and the variance of the previous steps.\n",
        "  - [More about optimizers](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop)\n",
        "  \n",
        "- **Metrics:**\n",
        "  - The training progress of the model is calculated in terms of metrics used\n",
        "  - Here, we monitor the progress of the training in terms of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDAUQu6CbbZD",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model\n",
        "- Model is trained on training images and its labels\n",
        "- Model is trained on batch_size of 32 that is, 32 images are passed through the network at a time\n",
        "- The model is trained on the dataset for 10 iterations (epochs)\n",
        "- Verbose 1 means it will print all the training information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzyuqACn35qx",
        "colab_type": "code",
        "outputId": "ad8bcead-1652-48fb-abb8-3e889f0f783c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=25, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/25\n",
            "60000/60000 [==============================] - 27s 448us/step - loss: 0.1299 - acc: 0.9600 - val_loss: 0.0519 - val_acc: 0.9827\n",
            "Epoch 2/25\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0484 - acc: 0.9853 - val_loss: 0.0486 - val_acc: 0.9861\n",
            "Epoch 3/25\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0481 - val_acc: 0.9883\n",
            "Epoch 4/25\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0290 - acc: 0.9907 - val_loss: 0.0247 - val_acc: 0.9928\n",
            "Epoch 5/25\n",
            "60000/60000 [==============================] - 17s 279us/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0363 - val_acc: 0.9898\n",
            "Epoch 6/25\n",
            "60000/60000 [==============================] - 17s 275us/step - loss: 0.0195 - acc: 0.9934 - val_loss: 0.0416 - val_acc: 0.9869\n",
            "Epoch 7/25\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0339 - val_acc: 0.9903\n",
            "Epoch 8/25\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.0413 - val_acc: 0.9899\n",
            "Epoch 9/25\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0128 - acc: 0.9960 - val_loss: 0.0291 - val_acc: 0.9915\n",
            "Epoch 10/25\n",
            "60000/60000 [==============================] - 17s 288us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.0309 - val_acc: 0.9921\n",
            "Epoch 11/25\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0413 - val_acc: 0.9918\n",
            "Epoch 12/25\n",
            "60000/60000 [==============================] - 16s 274us/step - loss: 0.0098 - acc: 0.9969 - val_loss: 0.0295 - val_acc: 0.9922\n",
            "Epoch 13/25\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0095 - acc: 0.9971 - val_loss: 0.0414 - val_acc: 0.9917\n",
            "Epoch 14/25\n",
            "60000/60000 [==============================] - 16s 273us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.0397 - val_acc: 0.9923\n",
            "Epoch 15/25\n",
            "60000/60000 [==============================] - 17s 282us/step - loss: 0.0096 - acc: 0.9971 - val_loss: 0.0395 - val_acc: 0.9912\n",
            "Epoch 16/25\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.0344 - val_acc: 0.9916\n",
            "Epoch 17/25\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0417 - val_acc: 0.9912\n",
            "Epoch 18/25\n",
            "60000/60000 [==============================] - 17s 284us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0365 - val_acc: 0.9918\n",
            "Epoch 19/25\n",
            "60000/60000 [==============================] - 16s 272us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0443 - val_acc: 0.9909\n",
            "Epoch 20/25\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0086 - acc: 0.9977 - val_loss: 0.0531 - val_acc: 0.9910\n",
            "Epoch 21/25\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0065 - acc: 0.9981 - val_loss: 0.0635 - val_acc: 0.9918\n",
            "Epoch 22/25\n",
            "60000/60000 [==============================] - 16s 271us/step - loss: 0.0079 - acc: 0.9980 - val_loss: 0.0400 - val_acc: 0.9927\n",
            "Epoch 23/25\n",
            "60000/60000 [==============================] - 16s 270us/step - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0456 - val_acc: 0.9920\n",
            "Epoch 24/25\n",
            "60000/60000 [==============================] - 16s 270us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.0436 - val_acc: 0.9920\n",
            "Epoch 25/25\n",
            "60000/60000 [==============================] - 17s 281us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0409 - val_acc: 0.9917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f524a775780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXma7Ya5bmD5",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate the Model\n",
        "- Performance of the model is evaluated on the tetsing datasets (images which the network has not seen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0_VWnvububV",
        "colab_type": "text"
      },
      "source": [
        "### Model Accuracy \n",
        "The accuracy of the model is **99.27%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-kmgZcKb2sr",
        "colab_type": "text"
      },
      "source": [
        "### Check all the prediction for testig dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyZF3gi-b4Dz",
        "colab_type": "text"
      },
      "source": [
        "### Print 10 predictions and labels (ground-truth)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "c483bc2c-0b03-4dcd-fb1c-fb979989e5c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6.1104111e-22 3.3940787e-19 1.2489525e-15 2.4552819e-14 1.3661450e-22\n",
            "  3.1407635e-16 2.1086242e-24 1.0000000e+00 4.6250811e-23 4.4672169e-14]\n",
            " [5.5005929e-13 1.8876578e-17 1.0000000e+00 1.8288546e-25 2.6324120e-20\n",
            "  1.8464444e-23 1.3839018e-12 6.8832987e-19 6.7708947e-20 1.4311431e-25]\n",
            " [3.0496469e-20 1.0000000e+00 4.2285341e-17 5.3767379e-25 2.0805079e-16\n",
            "  4.7837290e-14 7.1120220e-18 1.0227195e-14 4.6771512e-17 2.7996388e-17]\n",
            " [1.0000000e+00 2.5562475e-17 1.2272951e-12 2.0775224e-12 6.0768692e-14\n",
            "  4.2821002e-10 1.6427927e-10 7.9850948e-13 8.4694662e-14 6.7554462e-11]\n",
            " [2.2100075e-29 7.0998709e-20 8.2034017e-31 1.5319759e-32 1.0000000e+00\n",
            "  1.4377989e-30 3.0124361e-22 3.8705144e-27 6.4495334e-19 3.0503815e-19]\n",
            " [8.6331244e-20 1.0000000e+00 4.9556847e-17 1.1704958e-25 1.9920563e-14\n",
            "  1.5178024e-14 8.2773939e-17 3.1601048e-13 7.1576385e-18 4.1117015e-16]\n",
            " [2.1232021e-31 5.2415070e-14 3.1301658e-31 1.6606819e-31 1.0000000e+00\n",
            "  8.9133378e-24 6.0399176e-28 1.0317151e-24 7.5772093e-18 1.2125125e-17]\n",
            " [1.0536668e-23 1.2627782e-23 1.7985431e-19 5.5114701e-18 2.0580020e-10\n",
            "  3.0402879e-18 4.6808699e-22 1.0318260e-15 3.2666233e-17 1.0000000e+00]\n",
            " [5.7095923e-18 1.4696070e-21 2.5954613e-25 1.5209662e-11 7.3453215e-16\n",
            "  9.9999976e-01 6.1856353e-10 1.3017233e-26 2.7445364e-07 3.1311795e-09]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKwP8nJDcEO5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "### Visualization of Filters in a Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv_7'):\n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}